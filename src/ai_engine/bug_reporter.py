import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.ai_client import AIClient
from utils.config import config
from utils.logger import get_logger

logger = get_logger(__name__)

def generate_bugs_report(healing_analysis_path: Optional[str] = None) -> str:
    project_root: Path = config.get_project_root()

    if healing_analysis_path is None:
        healing_analysis_path = "reports/healing_analysis.json"

    analysis_path: Path = project_root / healing_analysis_path

    if not analysis_path.exists():
        logger.warning(f"Healing analysis file not found: {analysis_path}")
        return ""

    with open(analysis_path, "r") as f:
        analysis: Dict[str, Any] = json.load(f)

    actual_defects: List[Dict[str, Any]] = analysis.get("actual_defects", [])

    if not actual_defects:
        logger.info("No actual defects found - skipping BUGS.md generation")
        return ""

    logger.info(f"Generating detailed bug report for {len(actual_defects)} defect(s)...")

    client: AIClient = AIClient()

    timestamp: str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    bugs_md: str = f"""# Bug Report

**Generated**: {timestamp}
**Total Defects Found**: {len(actual_defects)}

---

The AI test automation framework identified **{len(actual_defects)} potential bug(s)** in the application during test execution. These failures were classified as ACTUAL_DEFECT (not test errors) and require manual investigation.

---

"""

    for idx, defect in enumerate(actual_defects, 1):
        test_name: str = defect.get("test_name", "Unknown")
        confidence: str = defect.get("confidence", "unknown")
        error: str = defect.get("error", "N/A")
        analysis_text: str = defect.get("analysis", "N/A")
        healing_attempts: int = defect.get("healing_attempts", 0)

        logger.info(f"Analyzing defect {idx}/{len(actual_defects)}: {test_name}")

        bugs_md += f"""### Bug #{idx}: {test_name}

**Confidence Level**: {confidence.upper()}
**Classification**: ACTUAL_DEFECT
"""

        if healing_attempts > 0:
            bugs_md += f"**Healing Attempts Before Classification**: {healing_attempts}  \n"

        bugs_md += f"""
**Error Message**:
```
{error}
```

**Initial AI Analysis**:
{analysis_text}

---

"""

        try:
            detailed_analysis: str = client.analyze_bug(defect)
            bugs_md += f"{detailed_analysis}\n\n---\n\n"
        except Exception as e:
            logger.error(f"Could not generate detailed analysis: {e}")
            bugs_md += f"*Detailed analysis unavailable*\n\n---\n\n"

    bugs_md += f"""## Next Steps

1. **Review Each Bug**: Examine the detailed analysis above
2. **Prioritize**: Focus on high-confidence, critical severity bugs first
3. **Investigate**: Follow the suggested investigation areas
4. **Reproduce**: Use the reproduction steps to confirm the bug
5. **Fix**: Implement fixes based on potential solutions
6. **Verify**: Re-run tests after fixes to confirm resolution

- These bugs were identified by AI analysis of test failures
- Confidence levels indicate AI's certainty about the classification
- Some bugs may be false positives - verify before fixing
- Tests that caught these bugs are NOT healed automatically
- Original test code preserved for bug reproduction

---

*Report generated by AI Test Automation Framework*
"""

    bugs_path: Path = project_root / "reports" / "BUGS.md"
    bugs_path.parent.mkdir(parents=True, exist_ok=True)

    with open(bugs_path, "w") as f:
        f.write(bugs_md)

    logger.info(f"Bug report saved to: {bugs_path}")

    return str(bugs_path)

if __name__ == "__main__":
    bugs_file: str = generate_bugs_report()
    if bugs_file:
        logger.info(f"Bug report generated: {bugs_file}")
    else:
        logger.info("No bugs to report")
